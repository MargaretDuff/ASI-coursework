---
title: "ASI Coursework"
author: "Margaret Duff"
date: "30 November 2018"

output:
  pdf_document: default
  word_document: default
  html_document: default
fontsize: 9pt
geometry: margin=1cm 

---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Analysis using Metropolis Hastings
```{r include=FALSE}
rats<- read.table("http://people.bath.ac.uk/kai21/ASI/rats_data.txt")
head(rats)
```
To make life easier later we split rats into the two categories based on their status 
```{r}
split_rats <- split(rats, rats$status)
tumour=as.data.frame(split_rats[[2]]); dead=as.data.frame(split_rats[[1]])
```
Let $T_i$ be the follow up time to tumor appearance in rat $i$.  Assume the following probability model
$$T_i\sim \mbox{Weibull}(\mbox{shape}=1/\sigma,\mbox{scale}=\exp(\eta_i))\,,\qquad \eta_i=\beta_0+\beta_1\,x_i$$
where
$$x_i=\left\{
\begin{array}{cl}
1 & \mbox{rat $i$ received treatment} \\
0 & \mbox{rat $i$ received control}\\
\end{array}
\right.$$
Assume that $\{T_1,\ldots,T_n\}$ are independent random variables.
The survival function of a Weibull with shape $a>0$ and scale $b>0$ is given by
$$S(t|a,b)=P(T>t|a,b)=\exp\left(-\left(\frac{t}{b}\right)^a\right)\,,\qquad t>0$$
We assume that  censored observations (\texttt{status=0}) do not contribute to the likelihood with a factor equal to the density of $T$ at the observed $t_i$ but with a factor equal to the survival function evaluated at the observed $t_i$.

Use a random walk Metropolis-Hastings algorithm to sample from the posterior distribution of $\theta=(\beta_0, \beta_1, \log(\sigma), \log(\sigma_b))^T$ using the model above.  We follow a Bayesian estimation procedure and specify the following prior distributions on the unknown parameters: $\beta_0$ $\beta_1$ and $\log(\sigma)$ are independent and following uniform (improper) priors while 
$\sigma_b$ is also independent of $\beta_0$ $\beta_1$ and $\log(\sigma)$ and follows a prior exponential distribution with rate $5$.  This gives us the following log posterior function 
```{r}
log.post <- function(beta0, beta1, log_sigma,log_sigma_b,b_tumour, b_dead,  tumour, dead) { 
  log_pi0_beta0=log(1);  log_pi0_beta1=  log(1);  log_pi0_log_sigma=log(1);  log_pi0_log_sigma_b= log(5*exp(-5*exp(log_sigma_b))) # log of the priors
    log_pi0_b_tumour=log(dnorm(b_tumour, mean=0, sd=exp(log_sigma_b)));  log_pio_b_dead=log(dnorm(b_dead, mean=0, sd=exp(log_sigma_b))) # log of the priors
  log_pi0<- log_pi0_beta0+log_pi0_beta1+log_pi0_log_sigma+log_pi0_log_sigma_b+sum(log_pio_b_dead)+sum(log_pi0_b_tumour)# we add the priors since we assume they are independent 
  log.lik<-sum(log (dweibull(tumour$time, 1/exp(log_sigma), exp(beta0+beta1*tumour$rx+b_tumour))))+sum(log (pweibull(dead$time, 1/exp(log_sigma), exp(beta0+beta1*dead$rx+b_dead))))# Now the log likelihood
  return(log.lik+log_pi0)# now the log posterior = log likelihood +log prior 
}
```
We use a normal centered on the current values as the proposal for $\beta_0$, $\beta_1$, $\log(\sigma_b)$,  $\log(sigma)$ and each element of $b$ I will also use a  symmetric multivariate normal distribution centered on the current values for the values of $b$.  I will need to tune the proposal standard deviations to get appropriate acceptance rates, aiming for about 25%.

The initial value is important since we would like to start in a region of the parameter space with high density as otherwise, that is, if the posterior density is extremely low for the initial value, it will take us a long time (a large number of generated values) to reach the area of high posterior density and therefore a long time to start sampling from the stationary distribution of the Markov chain. Thus we choose as our initial conditions the maximum likelihood estimators calculated earlier. 

We choose a burn in period of 10,000 but this can always be changed later. 
```{r}
nsteps=100000 ; burn.in=10000
MH<-function(beta0_0,beta1_0, log_sigma_0, log_sigma_b0, b_tumour0, b_dead0, sigma_beta0, sigma_beta1, sigma_log_sigma, sigma_log_sigma_b,   sigma_b, nsteps){ 
  accept <- rep(0,3) # set up locations to store values at each step 
  beta0 <- rep(0,nsteps) ;  beta1=rep(0,nsteps);  log_sigma=rep(0,nsteps);  log_sigma_b=rep(0,nsteps)
  b_tumour=matrix(0,nsteps,length(tumour$rx)) ;  b_dead=matrix(0,nsteps,length(dead$rx)) # set up locations to store values at each step 
  beta0[1] <- beta0_0 ;  beta1[1]=beta1_0;  log_sigma[1]=log_sigma_0;  log_sigma_b[1]=log_sigma_b0;  b_tumour[1,]=b_tumour0;  b_dead[1,]=b_dead0   #Set intial values
lp0 <- log.post(beta0_0,beta1_0, log_sigma_0, log_sigma_b0,b_tumour0, b_dead0,tumour, dead) # calculate log posterior for these intial values
for( i in 2:nsteps){ #MH loop
current_beta0=beta0[i-1] ;current_beta1=beta1[i-1];current_log_sigma=log_sigma[i-1];  current_log_sigma_b=log_sigma_b[i-1];current_b_tumour=b_tumour[i-1,]; current_b_dead=b_dead[i-1,] # set current values
proposed_log_sigma_b=current_log_sigma_b+rnorm(1,0,sigma_log_sigma_b)#update sigma_b 
lp1 <- log.post(current_beta0,current_beta1, current_log_sigma, proposed_log_sigma_b,current_b_tumour, current_b_dead,tumour, dead) # find log posterior of new values 
acc <- exp(min(0,lp1-lp0))
if (runif(1)>=acc| !is.finite(acc)){#reject
  b_tumour[i,] <- current_b_tumour ;  b_dead[i,]=current_b_dead;  beta0[i] <- current_beta0;  beta1[i]=current_beta1;  log_sigma[i]=current_log_sigma;  log_sigma_b[i]=current_log_sigma_b 
  lp1<- lp0 ## Return to the 'old' log posterior 
}else {#accept
  accept[1]=accept[1]+1 # keep track of number of acceptances
  log_sigma_b[i]=proposed_log_sigma_b #store found values 
  lp0 <- lp1 ## uldate old log posterior to the new one 
    proposed_b_tumour=current_b_tumour + rnorm( length(tumour$rx), mean=0, sd=sigma_b)#update b
    proposed_b_dead=current_b_dead+ rnorm( length(dead$rx), mean=0, sd=sigma_b)#update b
    lp1 <- log.post(current_beta0,current_beta1, current_log_sigma, proposed_log_sigma_b,proposed_b_tumour, proposed_b_dead,tumour, dead) # update log posterior
  acc <- exp(min(0,lp1-lp0))
  if (runif(1)>=acc | !is.finite(acc)){#reject
  b_tumour[i,] <- current_b_tumour ;  b_dead[i,]=current_b_dead;  beta0[i] <- current_beta0;  beta1[i]=current_beta1;  log_sigma[i]=current_log_sigma     #store values 
  lp1<- lp0 ## Return to previous log posterior
}else {#accept
          accept[2]=accept[2]+1 # keep track to calculate acceptance rates
          b_tumour[i,] <- proposed_b_tumour;  b_dead[i,]=proposed_b_dead;  #store values   
          lp0 <- lp1 ## update log posterior 
  proposed_beta0=current_beta0+rnorm(1,0,sigma_beta0); proposed_beta1=current_beta1+rnorm(1,0,sigma_beta1)
  proposed_log_sigma=current_log_sigma+rnorm(1,0,sigma_log_sigma)#update remaining paramaters
  lp1=log.post(proposed_beta0,proposed_beta1, proposed_log_sigma, proposed_log_sigma_b,proposed_b_tumour, proposed_b_dead,tumour, dead) # calculate new log posterior
  acc <- exp(min(0,lp1-lp0))
  if (runif(1)>=acc| !is.finite(acc)){#reject
  beta0[i] <- current_beta0 ;  beta1[i]=current_beta1;  log_sigma[i]=current_log_sigma #store values
  lp1<- lp0 ## Return to previous log posterior
}else {#accept
    accept[3]=accept[3]+1 # keep track to calculate acceptance rates
  beta0[i] <- proposed_beta0;  beta1[i]=proposed_beta1;  log_sigma[i]=proposed_log_sigma#store values 
  lp0=lp1 # update log posterior 
}}}}
  list(beta0=beta0, beta1=beta1, log_sigma_b=log_sigma_b, log_sigma=log_sigma,ar_outer=accept[1]/nsteps, ar_middle=accept[2]/accept[1], ar_inner=accept[3]/accept[2])
}
mh=MH(5.0188886, -0.2376741, -1.3687252,-1.5976068, rep(0, length(tumour$rx)),rep(0, length(dead$rx)),0.1,0.1,0.1,0.2,0.001, nsteps = nsteps)
```
For tuning the proposal distribution we use run lengths of about 10000. 100000 would be
better for the final run. The aim is for an acceptance rate of approximately 25%.
```{r}
mh$ar_outer;mh$ar_middle;mh$ar_inner
```
We check for correlation between the parameters by plotting graphs.

```{r echo=FALSE}
par(mfrow=c(3,2),mar=c(4,4,1,1))
samp<-sample((1:nsteps)[-(burn.in)],nsteps/2) # For visualization purposes we take a random sample of the iterations retained (after discarding burn-in)
plot(mh$beta0[samp],mh$beta1[samp],xlab=expression(beta_0),ylab=expression(beta_1),pch=".",cex=0.1)
plot(mh$beta0[samp],mh$log_sigma[samp],xlab=expression(beta_0),ylab=expression(log(sigma)),pch=".",cex=0.1)
plot(mh$beta0[samp],mh$log_sigma_b[samp],xlab=expression(beta_0),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
plot(mh$beta1[samp],mh$log_sigma[samp],xlab=expression(beta_1),ylab=expression(log(sigma)),pch=".",cex=0.1)
plot(mh$beta1[samp],mh$log_sigma_b[samp],xlab=expression(beta_1),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
plot(mh$log_sigma[samp],mh$log_sigma_b[samp],xlab=expression(log(sigma)),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
```
We note the elliptical shaped graphs especially for the plots of $\beta_0, \beta_1,\log(\sigma), \log(\sigma_b)$. This suggests that the posterior density for $\theta$ is highly non independent, and independent jumps for each component will give slow mixing. We consider using a shrunken version of the co-variance, found using the results above, as the basis for proposing multivariate normal jumps in   a random walk i.e. $\theta_i\sim N(\theta_{i-1}, \lambda*\Sigma)$, where $\lambda>0$ and we can tune. To find the co-variance matrix, $\Sigma$: 
```{r}
library(mvtnorm)
mu=c(mean(mh$beta0[-(burn.in)]),mean(mh$beta1[-(burn.in)]), mean(mh$log_sigma[-(burn.in)]) )
s11=cov(mh$beta0[-(burn.in)], mh$beta0[-(burn.in)]);s12=cov(mh$beta0[-(burn.in)], mh$beta1[-(burn.in)])
s13=cov(mh$beta0[-(burn.in)], mh$log_sigma[-(burn.in)]);s22=cov(mh$beta1[-(burn.in)], mh$beta1[-(burn.in)])
s23=cov(mh$beta1[-(burn.in)], mh$log_sigma[-(burn.in)]);s33=cov(mh$log_sigma, mh$log_sigma)
covariance= matrix(c(s11,s12,s13,s12,s22,s23,s13,s23,s33), nrow=3, byrow=TRUE)
```
With this new proposal distribution for $(\beta_0, \beta_1, \log(\sigma)$ we get a new MH sampler identical to the previous one, except that when we propose new values in the inner most MH chain we use this excerpt of code:

```{r, eval=FALSE}
proposed= rmvnorm(1, mean=c(current_beta0, current_beta1, current_log_sigma), lambda*covariance) #update remaining paramaters
proposed_beta0=proposed[1];  proposed_beta1=proposed[2];  proposed_log_sigma=proposed[3]
```

```{r echo=FALSE}
nsteps=100000; burn.in=10000
MH2<-function(beta0_0,beta1_0, log_sigma_0, log_sigma_b0, b_tumour0, b_dead0, lambda, sigma_log_sigma_b,   sigma_b, nsteps){ 
  accept <- rep(0,3) # for acceptance values of outer, middle and inner chain
  beta0 <- rep(0,nsteps) ;  beta1=rep(0,nsteps)# set up locations to store values at each step 
  log_sigma=rep(0,nsteps);  log_sigma_b=rep(0,nsteps)# set up locations to store values at each step 
  b_tumour=matrix(0,nsteps,length(tumour$rx)) ;  b_dead=matrix(0,nsteps,length(dead$rx)) # set up locations to store values at each step 
  beta0[1] <- beta0_0 ;  beta1[1]=beta1_0;  log_sigma[1]=log_sigma_0;  log_sigma_b[1]=log_sigma_b0;  b_tumour[1,]=b_tumour0;  b_dead[1,]=b_dead0  #Set intial values
lp0 <- log.post(beta0_0,beta1_0, log_sigma_0, log_sigma_b0,b_tumour0, b_dead0,tumour, dead) # log posterior for initial values 
for( i in 2:nsteps){ #MH loop
     current_beta0=beta0[i-1];    current_beta1=beta1[i-1]# set current values 
    current_log_sigma=log_sigma[i-1];    current_log_sigma_b=log_sigma_b[i-1]# set current values 
    current_b_tumour=b_tumour[i-1,];    current_b_dead=b_dead[i-1,]# set current values 
proposed_log_sigma_b=current_log_sigma_b+rnorm(1,0,sigma_log_sigma_b)#update sigma_b 
lp1 <- log.post(current_beta0,current_beta1, current_log_sigma, proposed_log_sigma_b,current_b_tumour, current_b_dead,tumour, dead)#new log posterior
    acc <- exp(min(0,lp1-lp0))
if (runif(1)>=acc| !is.finite(acc)){#reject
  b_tumour[i,] <- current_b_tumour;  b_dead[i,]=current_b_dead;  beta0[i] <- current_beta0;  beta1[i]=current_beta1;  log_sigma[i]=current_log_sigma;  log_sigma_b[i]=current_log_sigma_b # store all values
  lp1<- lp0##make sure correct log posteriors are stored 
}else {#accept
  accept[1]=accept[1]+1 # keep track to caluclate acceptance rates
  log_sigma_b[i]=proposed_log_sigma_b #store found values 
  lp0 <- lp1 ##make sure correct log posteriors are stored 
    proposed_b_tumour=current_b_tumour + rnorm( length(tumour$rx), mean=0, sd=sigma_b)#update b
    proposed_b_dead=current_b_dead+ rnorm( length(dead$rx), mean=0, sd=sigma_b)#update b
    lp1 <- log.post(current_beta0,current_beta1, current_log_sigma, proposed_log_sigma_b,proposed_b_tumour, proposed_b_dead,tumour, dead) # new log posterior
  acc <- exp(min(0,lp1-lp0))
  if (runif(1)>=acc| !is.finite(acc)){#reject
  b_tumour[i,] <- current_b_tumour; b_dead[i,]=current_b_dead;  beta0[i] <- current_beta0;  beta1[i]=current_beta1; log_sigma[i]=current_log_sigma #store values 
  lp1<- lp0 ##make sure correct log posteriors are stored 
}else {#accept
  accept[2]=accept[2]+1# keep track to caluclate acceptance rates
  b_tumour[i,] <- proposed_b_tumour; b_dead[i,]=proposed_b_dead;log_sigma_b[i]=proposed_log_sigma_b  #store found values 
  lp0 <- lp1 ##make sure correct log posteriors are stored 
  proposed= rmvnorm(1, mean=c(current_beta0, current_beta1, current_log_sigma), lambda*covariance) #update remaining paramaters
  proposed_beta0=proposed[1];  proposed_beta1=proposed[2];  proposed_log_sigma=proposed[3]
  lp1=log.post(proposed_beta0,proposed_beta1, proposed_log_sigma, proposed_log_sigma_b,proposed_b_tumour, proposed_b_dead,tumour, dead)# new log posterior
  acc <- exp(min(0,lp1-lp0))
  if (runif(1)>=acc| !is.finite(acc)){#reject
  beta0[i] <- current_beta0;  beta1[i]=current_beta1;  log_sigma[i]=current_log_sigma #store values 
  lp1<- lp0 ##make sure correct log posteriors are stored 
}else {#accept
    accept[3]=accept[3]+1# keep track to caluclate acceptance rates
  beta0[i] <- proposed_beta0;  beta1[i]=proposed_beta1;  log_sigma[i]=proposed_log_sigma#store found values 
  lp0=lp1##make sure correct log posteriors are stored 
}}}}
  list(beta0=beta0, beta1=beta1, log_sigma_b=log_sigma_b, log_sigma=log_sigma,ar_outer=accept[1]/nsteps, ar_middle=accept[2]/accept[1], ar_inner=accept[3]/accept[2])
}
```
Using the new MH algorithm and tuning we get
```{r}
mh=MH2(5.0188886, -0.2376741, -1.3687252,-1.5976068, b_tumour0 =  rep(0, length(tumour$rx)),b_dead0= rep(0, length(dead$rx)),lambda=2.5,sigma_log_sigma_b = 0.2, sigma_b =0.001, nsteps = nsteps)
mh$ar_outer;mh$ar_middle;mh$ar_inner
```

We now check that the Markov chain is behaving as we expect. Firstly, the trace plots show some good mixing within the parameters

```{r echo=FALSE, fig.height=3}
show.plot<- (burn.in):nsteps
par(mfrow=c(2,2),mar=c(3,4,1,1))
plot(mh$beta0[show.plot],type="l",ylab=expression(beta0))
plot(mh$beta1[show.plot],type="l",ylab=expression(beta1))
plot(mh$log_sigma[show.plot],type="l",ylab=expression(log(sigma)))
plot(mh$log_sigma_b[show.plot],type="l",ylab=expression(log(sigma_b)))
```

The auto-correlation function plots are also decreasing suggesting that the Markov chain is converging. 

```{r echo=FALSE, fig.height=3}
par(mfrow=c(2,2),mar=c(4,4,1,1))
acf(mh$beta0[-burn.in],xlab=expression(beta0))
acf(mh$beta1[-burn.in],xlab=expression(beta1))
acf(mh$log_sigma[-burn.in],xlab=expression(log(sigma)))
acf(mh$log_sigma_b[-burn.in],xlab=expression(log(sigma_b)))
```

We can also get some idea of the effective sample size for each of our parameters
```{r}
n.eff <- c(0,0,0,0)
autocor <- acf(mh$beta0[-(burn.in)],plot=FALSE); t.eff <- 2*sum(autocor[[1]]) - 1; n.eff[1] <- nsteps/t.eff
autocor <- acf(mh$beta1[-(burn.in)],plot=FALSE); t.eff <- 2*sum(autocor[[1]]) - 1; n.eff[2] <- nsteps/t.eff
autocor <- acf(mh$log_sigma[-(burn.in)],plot=FALSE); t.eff <- 2*sum(autocor[[1]]) - 1;n.eff[3] <- nsteps/t.eff
autocor <- acf(mh$log_sigma_b[-(burn.in)],plot=FALSE);t.eff <- 2*sum(autocor[[1]]) - 1 ;n.eff[4] <- nsteps/t.eff
n.eff
```
We will also obtain an independent sub-sample of our chain and then split it into two sub-samples. Using the Kolmogorov-Smirnov test we chance to see that they come from the same distribution and hence that the Markov chain has converged. 
```{r message=FALSE, warning=FALSE}
k1=ks.test(mh$beta0[-1000:-500], mh$beta0[-500]); 
k2=ks.test(mh$beta1[-1000:-500], mh$beta1[-500]);
k3=ks.test(mh$log_sigma_b[-1000:-500], mh$log_sigma_b[-500]);
k4=ks.test(mh$log_sigma[-1000:-500], mh$log_sigma[-500]);
c(k1$p.value,k2$p.value,k3$p.value,k4$p.value)
```
We see that in all cases the p-values are large, and thus there is not significant evidence that the two sub-samples are from different distributions. Hence we conclude that the Markov chain has converged. 

Again, we investigate the correlation between the parameters by plotting a random sample of the iterations retained after discarding the burn in period . 

```{r echo=FALSE}
par(mfrow=c(3,2),mar=c(4,4,1,1))
samp<-sample((1:nsteps)[-(burn.in)],nsteps/2)# For visualization purposes we take a random sample of the iterations retained (after discarding burn-in)
plot(mh$beta0[samp],mh$beta1[samp],xlab=expression(beta_0),ylab=expression(beta_1),pch=".",cex=0.1)
plot(mh$beta0[samp],mh$log_sigma[samp],xlab=expression(beta_0),ylab=expression(log(sigma)),pch=".",cex=0.1)
plot(mh$beta0[samp],mh$log_sigma_b[samp],xlab=expression(beta_0),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
plot(mh$beta1[samp],mh$log_sigma[samp],xlab=expression(beta_1),ylab=expression(log(sigma)),pch=".",cex=0.1)
plot(mh$beta1[samp],mh$log_sigma_b[samp],xlab=expression(beta_1),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
plot(mh$log_sigma[samp],mh$log_sigma_b[samp],xlab=expression(log(sigma)),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
```
It looks like we need a more sophisticated proposal to deal with the correlation of $\log(\sigma_b)$ and the other 3 parameters. 

We also investigate the shape of the marginal posterior densities of the parameters by plotting histograms of the retained values 

```{r echo=FALSE, fig.height=3}
par(mfrow=c(2,2),mar=c(4,4,1,1))
hist(mh$beta0[-burn.in], freq = FALSE, breaks = 100)
hist(mh$beta1[-burn.in], freq = FALSE, breaks = 100)
hist(mh$log_sigma[-burn.in], freq = FALSE, breaks = 100)
hist(mh$log_sigma_b[-burn.in], freq = FALSE, breaks = 100)
```
The long non symmetric tails on some of the marginal posterior densities suggesting such as on the $\log(\sigma_b)$ graph again suggest a more sophisticated proposal would be useful. 
We compute a 95% posterior probability interval for the intervention effect $\beta_1$. 
```{r}
quantile(mh$beta1[-(burn.in)], c(.025, 0.975))#95% condfidence interval for beta_1
```
We conclude that 0 is not contained in the 95% posterior probability interval for the intervention effect, suggesting that the intervention has a statistically significant effect. 