---
output:
  word_document: default
  pdf_document: default
  html_document: default
---
title: "ASI Coursework"
author: "Margaret Duff"
date: "30 November 2018"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Carcinogenesis study on rats

Consider the following data about a drug trial on 50 litters of female rats. There were three rats per litter, one that received a drug treatment and two received a  control (placebo). The variables are:

* \texttt{litter} is the litter number

* \texttt{rx} is the treatment indicator where \texttt{rx=1} indicates the treatment and \texttt{rx=0} indicates the control 

* \texttt{time} contains the follow up time (in weeks) to tumor appearance

* \texttt{status} is an indicator variable of the presence of  tumor \texttt{status=1} or not \texttt{status=0} which indicates censoring  due to animal death before tumor appearance. 


The data can be read as follows
```{r}
rats<- read.table("http://people.bath.ac.uk/kai21/ASI/rats_data.txt")
head(rats)
```
To make life easier later we split rats into the two categories based on status 

```{r}
split_rats <- split(rats, rats$status)
tumour=as.data.frame(split_rats[[2]])
dead=as.data.frame(split_rats[[1]])

head(tumour)
head(dead)

```

Let $T_i$ be the follow up time to tumor appearance in rat $i$.  Assume the following probability model
$$T_i\sim \mbox{Weibull}(\mbox{shape}=1/\sigma,\mbox{scale}=\exp(\eta_i))\,,\qquad \eta_i=\beta_0+\beta_1\,x_i$$
where
$$x_i=\left\{
\begin{array}{cl}
1 & \mbox{rat $i$ received treatment} \\
0 & \mbox{rat $i$ received control}\\
\end{array}
\right.$$
Assume that $\{T_1,\ldots,T_n\}$ are independent random variables.
The survival function of a Weibull with shape $a>0$ and scale $b>0$ is given by
$$S(t|a,b)=P(T>t|a,b)=\exp\left(-\left(\frac{t}{b}\right)^a\right)\,,\qquad t>0$$
You may use the functions \texttt{dweibull} and \texttt{pweibull} in R. You should assume that  censored observations (\texttt{status=0}) do not contribute to the likelihood with a factor equal to the density of $T$ at the observed $t_i$ but with a factor equal to the survival function evaluated at the observed $t_i$.

5. Using the same model as in question 4, now assume we follow a Bayesian estimation procedure and specify the following prior distributions on the unknown parameters: $\beta_0$ $\beta_1$ and $\log(\sigma)$ are independent and following uniform (improper) priors while 
$\sigma_b$ is also independent of $\beta_0$ $\beta_1$ and $\log(\sigma)$ and follows a prior exponential distribution with rate $5$. 
Use a random walk Metropolis-Hastings algorithm to sample from the posterior distribution of $\bm{\theta}$ using the following points as guidelines:
```{r}
# function to compute log posterior 
log.post <- function(beta0, beta1, log_sigma,log_sigma_b,b_tumour, b_dead,  tumour, dead) { 
  x_tumour=tumour$rx
  y_tumour=tumour$time
  litter_tumour=tumour$litter
  n_tumour=length(y_tumour)
  x_dead=dead$rx
  y_dead=dead$time
  n_dead=length(y_dead)
  litter_dead=dead$litter
  log_pi0_beta0=log(1)
  log_pi0_beta1=  log(1)
  log_pi0_log_sigma=log(1)
  log_pi0_log_sigma_b= log(5*exp(-5*exp(log_sigma_b)))
  
  
  log_pi0_b_tumour=log(dnorm(b_tumour, mean=0, sd=exp(log_sigma_b)))
  log_pio_b_dead=log(dnorm(b_dead, mean=0, sd=exp(log_sigma_b)))
  
   # we add the priors since we assume they are independent 
  log_pi0<- log_pi0_beta0+log_pi0_beta1+log_pi0_log_sigma+log_pi0_log_sigma_b+sum(log_pio_b_dead)+sum(log_pi0_b_tumour)

  # Now the log likelihood
  log.lik<-sum(log (dweibull(y_tumour, 1/exp(log_sigma), exp(beta0+beta1*x_tumour+b_tumour))))+sum(log (pweibull(y_dead, 1/exp(log_sigma), exp(beta0+beta1*x_dead+b_dead))))
  
  # now the log posterior = log likelihood +log prior 
  return(log.lik+log_pi0)
}
log.post(5,-0.2,-1.3,-1.5,rep(0, length(tumour$rx)),rep(0, length(dead$rx)),tumour, dead)
```




* Specify clearly the proposal distributions used.

We use a normal centred on the current values as the proposal for $\beta_0$, $\beta_1$, $\log(\sigma_b)$ and $\log(sigma)$. I will also use a  symmetric multivariate normal distribution centred on the current values for the values of $b$.  I will need to tune the proposal standard deviation to get appropriate acceptance rates, aiming for about 25%.




* Choose suitable starting values for the parameters and determine a suitable burn-in period in which all samples are discarded.

The initial value is important since we would like to start in a region of the parameter space with high density as otherwise, that is, if the posterior density is extremely low for the initial value, it will take us a long time (a large number of generated values) to reach the area of high posterior density and therefore a long time to start sampling from the stationary distribution of the Markov chain. Thus we choose as our initial conditions the maximum likelihood estimators calculated earlier. 

We choose a burn in period of THIS HERE 

```{r}
nsteps=10000
burn.in=1000

MH<-function(beta0_0,beta1_0, log_sigma_0, log_sigma_b0, b_tumour0, b_dead0, sigma_beta0, sigma_beta1, sigma_log_sigma, sigma_log_sigma_b,   sigma_b, nsteps){ 
# set up locations to store values at each step 
  accept <- rep(0,3)
  beta0 <- rep(0,nsteps)
  beta1=rep(0,nsteps)
  log_sigma=rep(0,nsteps)
  log_sigma_b=rep(0,nsteps)
  b_tumour=matrix(0,nsteps,length(tumour$rx)) 
  b_dead=matrix(0,nsteps,length(dead$rx)) 
  
  #Set intial values
  beta0[1] <- beta0_0
  beta1[1]=beta1_0
  log_sigma[1]=log_sigma_0
  log_sigma_b[1]=log_sigma_b0
  b_tumour[1,]=b_tumour0
  b_dead[1,]=b_dead0
  
lp0 <- log.post(beta0_0,beta1_0, log_sigma_0, log_sigma_b0,b_tumour0, b_dead0,tumour, dead)

for( i in 2:nsteps){ #MH loop
  
# set current values 
    current_beta0=beta0[i-1]
    current_beta1=beta1[i-1]
    current_log_sigma=log_sigma[i-1]
    current_log_sigma_b=log_sigma_b[i-1]
    current_b_tumour=b_tumour[i-1,]
    current_b_dead=b_dead[i-1,]

#update sigma_b 
proposed_log_sigma_b=current_log_sigma_b+rnorm(1,0,sigma_log_sigma_b)
lp1 <- log.post(current_beta0,current_beta1, current_log_sigma, proposed_log_sigma_b,current_b_tumour, current_b_dead,tumour, dead)

    acc <- exp(min(0,lp1-lp0))
    
if (runif(1)>=acc){#reject
  b_tumour[i,] <- current_b_tumour
  b_dead[i,]=current_b_dead
  beta0[i] <- current_beta0
  beta1[i]=current_beta1
  log_sigma[i]=current_log_sigma
  log_sigma_b[i]=current_log_sigma_b 
  lp1<- lp0 ## Keep ll0 in sync with th
}else {#accept
  #store found values 
  accept[1]=accept[1]+1
            log_sigma_b[i]=proposed_log_sigma_b 
          lp0 <- lp1 ## Keep ll0 in sync with th
  
  #update b
      proposed_b_tumour=current_b_tumour + rnorm( length(tumour$rx), mean=0, sd=sigma_b)
    proposed_b_dead=current_b_dead+ rnorm( length(dead$rx), mean=0, sd=sigma_b)
    lp1 <- log.post(current_beta0,current_beta1, current_log_sigma, proposed_log_sigma_b,proposed_b_tumour, proposed_b_dead,tumour, dead)
    check=is.finite(lp1)
  acc <- exp(min(0,lp1-lp0))
  if (runif(1)>=acc){#reject
    #store values 
  b_tumour[i,] <- current_b_tumour
  b_dead[i,]=current_b_dead
  beta0[i] <- current_beta0
  beta1[i]=current_beta1
  log_sigma[i]=current_log_sigma
  lp1<- lp0 ## Keep ll0 in sync with th
}else {#accept
    #store values 
          accept[2]=accept[2]+1
          b_tumour[i,] <- proposed_b_tumour
          b_dead[i,]=proposed_b_dead
          log_sigma_b[i]=proposed_log_sigma_b 
          lp0 <- lp1 ## Keep ll0 in sync with th

  
  #update remaining paramaters
  proposed_beta0=current_beta0+rnorm(1,0,sigma_beta0)
  proposed_beta1=current_beta1+rnorm(1,0,sigma_beta1)
  proposed_log_sigma=current_log_sigma+rnorm(1,0,sigma_log_sigma)
  lp1=log.post(proposed_beta0,proposed_beta1, proposed_log_sigma, proposed_log_sigma_b,proposed_b_tumour, proposed_b_dead,tumour, dead)
              
  acc <- exp(min(0,lp1-lp0))
              
  if (runif(1)>=acc){#reject
    #store values 

  beta0[i] <- current_beta0
  beta1[i]=current_beta1
  log_sigma[i]=current_log_sigma
  lp1<- lp0 ## Keep ll0 in sync with th  
}else {#accept
  
  #store values 
    accept[3]=accept[3]+1
  beta0[i] <- proposed_beta0
  beta1[i]=proposed_beta1
  log_sigma[i]=proposed_log_sigma
  lp0=lp1
  
}
}

}
}
  list(beta0=beta0, beta1=beta1, log_sigma_b=log_sigma_b, log_sigma=log_sigma,ar_outer=accept[1]/nsteps, ar_middle=accept[2]/accept[1], ar_inner=accept[3]/accept[2])
}

mh=MH(5,-0.2,-1.3,-1.5, rep(0, length(tumour$rx)),rep(0, length(dead$rx)),0.1,0.1,0.1,0.2,0.001, nsteps = nsteps)
mh$ar_outer
mh$ar_middle
mh$ar_inner

```



* Tune the proposal standard deviations.

For tuning the proposal distribution we use run lengths of about 10000. 100000 would be
better for the final run. The aim is for an acceptance rate of approximately 25%


* After running the tuned Metropolis-Hastings sampler check for correlation between the parameters.

```{r}
show.plot<- (burn.in):nsteps
par(mfrow=c(4,1),mar=c(3,4,1,1))
plot(mh$beta0[show.plot],type="l",ylab=expression(beta0))
plot(mh$beta1[show.plot],type="l",ylab=expression(beta1))
plot(mh$log_sigma[show.plot],type="l",ylab=expression(log(sigma)))
plot(mh$log_sigma_b[show.plot],type="l",ylab=expression(log(sigma_b)))
```

```{r}

par(mfrow=c(2,2),mar=c(4,4,1,1))
acf(mh$beta0[-burn.in],xlab=expression(beta0))
acf(mh$beta1[-burn.in],xlab=expression(beta1))
acf(mh$log_sigma[-burn.in],xlab=expression(log(sigma)))
acf(mh$log_sigma_b[-burn.in],xlab=expression(log(sigma_b)))
```

calculate the effective sample size for each paramater and see that the effective smaple size for $\log(\sigma)$ are about half the size than those for $\beta_0$, $\beta_1$ and $\sigma_b$.  
```{r}
# note we do discard burn-in iterations
n.eff <- c(0,0,0,0)
###
autocor <- acf(mh$beta0[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[1] <- nsteps/t.eff
###
autocor <- acf(mh$beta1[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[2] <- nsteps/t.eff
###
autocor <- acf(mh$log_sigma[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[3] <- nsteps/t.eff
###
autocor <- acf(mh$log_sigma_b[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[4] <- nsteps/t.eff

# t.eff is the integrated autocrrelation length
n.eff
```


We check for correlation between the parameters by plotting graphs... 

```{r}
par(mfrow=c(3,2),mar=c(4,4,1,1))
# For visualization purposes we take a random sample
# of the iterations retained (after discarding burn-in)
samp<-sample((1:nsteps)[-(burn.in)],nsteps/2)

plot(mh$beta0[samp],mh$beta1[samp],xlab=expression(beta_0),ylab=expression(beta_1),pch=".",cex=0.1)

plot(mh$beta0[samp],mh$log_sigma[samp],xlab=expression(beta_0),ylab=expression(log(sigma)),pch=".",cex=0.1)

plot(mh$beta0[samp],mh$log_sigma_b[samp],xlab=expression(beta_0),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
plot(mh$beta1[samp],mh$log_sigma[samp],xlab=expression(beta_1),ylab=expression(log(sigma)),pch=".",cex=0.1)
plot(mh$beta1[samp],mh$log_sigma_b[samp],xlab=expression(beta_1),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
plot(mh$log_sigma[samp],mh$log_sigma_b[samp],xlab=expression(log(sigma)),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
```



* Write a new Metropolis-Hastings sampler that makes use of any posterior correlation between the parameters (you may find the \texttt{mvtnorm} package in R helpful for this). See section 6.5.1 of \emph{Core statistics}.


Fit a multivariate normal distribution to the paramaters from thw inner metropolis hastings step 

```{r}
mu=c(mean(mh$beta0[-(burn.in)]),mean(mh$beta1[-(burn.in)]), mean(mh$log_sigma[-(burn.in)]) )
s11=cov(mh$beta0, mh$beta0)
s12=cov(mh$beta0, mh$beta1)
s13=cov(mh$beta0, mh$log_sigma)
s22=cov(mh$beta1, mh$beta1)
s23=cov(mh$beta1, mh$log_sigma)
s33=cov(mh$log_sigma, mh$log_sigma)

covariance= matrix(c(s11,s12,s13,s12,s22,s23,s13,s23,s33), nrow=3, byrow=TRUE)

mu
covariance

library(mvtnorm)

rmvt(1, covariance, df=0, delta=mu)
```

Not try writing a new MH algorithm using $\theta |y \sim N(\mu, \Sigma)$.

This we now have the case of an independence sampler. We peopose values of $\theta$ using the fixed distribution $\theta' \sim N(\theta_{j-1}, \Sigma)$. 

$$ \alpha(\theta'|\theta_{j-1})=\min\{1, \frac{\pi(\theta')q(\theta_{j-1}|\theta')}{\pi(\theta_{j-1})q(\theta'|\theta_{j-1})}\}   $$

```{r}
nsteps=1000
burn.in=100

MH2<-function(beta0_0,beta1_0, log_sigma_0, log_sigma_b0, b_tumour0, b_dead0, lambda, sigma_log_sigma_b,   sigma_b, nsteps){ 
# set up locations to store values at each step 
  accept <- rep(0,3)
  beta0 <- rep(0,nsteps)
  beta1=rep(0,nsteps)
  log_sigma=rep(0,nsteps)
  log_sigma_b=rep(0,nsteps)
  b_tumour=matrix(0,nsteps,length(tumour$rx)) 
  b_dead=matrix(0,nsteps,length(dead$rx)) 
  
  #Set intial values
  beta0[1] <- beta0_0
  beta1[1]=beta1_0
  log_sigma[1]=log_sigma_0
  log_sigma_b[1]=log_sigma_b0
  b_tumour[1,]=b_tumour0
  b_dead[1,]=b_dead0
  
lp0 <- log.post(beta0_0,beta1_0, log_sigma_0, log_sigma_b0,b_tumour0, b_dead0,tumour, dead)

for( i in 2:nsteps){ #MH loop
  
# set current values 
    current_beta0=beta0[i-1]
    current_beta1=beta1[i-1]
    current_log_sigma=log_sigma[i-1]
    current_log_sigma_b=log_sigma_b[i-1]
    current_b_tumour=b_tumour[i-1,]
    current_b_dead=b_dead[i-1,]

#update sigma_b 
proposed_log_sigma_b=current_log_sigma_b+rnorm(1,0,sigma_log_sigma_b)
lp1 <- log.post(current_beta0,current_beta1, current_log_sigma, proposed_log_sigma_b,current_b_tumour, current_b_dead,tumour, dead)

    acc <- exp(min(0,lp1-lp0))
    
if (runif(1)>=acc){#reject
  b_tumour[i,] <- current_b_tumour
  b_dead[i,]=current_b_dead
  beta0[i] <- current_beta0
  beta1[i]=current_beta1
  log_sigma[i]=current_log_sigma
  log_sigma_b[i]=current_log_sigma_b 
  lp1<- lp0 ## Keep ll0 in sync with th
}else {#accept
  #store found values 
  accept[1]=accept[1]+1
            log_sigma_b[i]=proposed_log_sigma_b 
          lp0 <- lp1 ## Keep ll0 in sync with th
  
  #update b
      proposed_b_tumour=current_b_tumour + rnorm( length(tumour$rx), mean=0, sd=sigma_b)
    proposed_b_dead=current_b_dead+ rnorm( length(dead$rx), mean=0, sd=sigma_b)
    lp1 <- log.post(current_beta0,current_beta1, current_log_sigma, proposed_log_sigma_b,proposed_b_tumour, proposed_b_dead,tumour, dead)
    check=is.finite(lp1)
  acc <- exp(min(0,lp1-lp0))
  if (runif(1)>=acc){#reject
    #store values 
  b_tumour[i,] <- current_b_tumour
  b_dead[i,]=current_b_dead
  beta0[i] <- current_beta0
  beta1[i]=current_beta1
  log_sigma[i]=current_log_sigma
  lp1<- lp0 ## Keep ll0 in sync with th
}else {#accept
    #store values 
          accept[2]=accept[2]+1
          b_tumour[i,] <- proposed_b_tumour
          b_dead[i,]=proposed_b_dead
          log_sigma_b[i]=proposed_log_sigma_b 
          lp0 <- lp1 ## Keep ll0 in sync with th

  
  #update remaining paramaters
  proposed= c(current_beta0, current_beta1, current_log_sigma_b)+lambda*rmvt(1, covariance, df=0, delta=c(0,0,0))
  proposed_beta0=proposed[1]
  proposed_beta1=proposed[2]
  proposed_log_sigma=proposed[3]
  lp1=log.post(proposed_beta0,proposed_beta1, proposed_log_sigma, proposed_log_sigma_b,proposed_b_tumour, proposed_b_dead,tumour, dead)
              
  acc <- exp(min(0,lp1-lp0))
              
  if (runif(1)>=acc){#reject
    #store values 

  beta0[i] <- current_beta0
  beta1[i]=current_beta1
  log_sigma[i]=current_log_sigma
  lp1<- lp0 ## Keep ll0 in sync with th  
}else {#accept
  
  #store values 
    accept[3]=accept[3]+1
  beta0[i] <- proposed_beta0
  beta1[i]=proposed_beta1
  log_sigma[i]=proposed_log_sigma
  lp0=lp1
  
}
}

}
}
  list(beta0=beta0, beta1=beta1, log_sigma_b=log_sigma_b, log_sigma=log_sigma,ar_outer=accept[1]/nsteps, ar_middle=accept[2]/accept[1], ar_inner=accept[3]/accept[2])
}

mh=MH2(5,-0.2,-1.3,-1.5, rep(0, length(tumour$rx)),rep(0, length(dead$rx)),0.01,0.2,0.0001, nsteps = nsteps)
mh$ar_outer
mh$ar_middle
mh$ar_inner

```

* After running the tuned Metropolis-Hastings sampler check for correlation between the parameters.

```{r}
show.plot<- (nsteps-1000):nsteps
par(mfrow=c(4,1),mar=c(3,4,1,1))
plot(mh$beta0[show.plot],type="l",ylab=expression(beta0))
plot(mh$beta1[show.plot],type="l",ylab=expression(beta1))
plot(mh$log_sigma[show.plot],type="l",ylab=expression(log(sigma)))
plot(mh$log_sigma_b[show.plot],type="l",ylab=expression(log(sigma_b)))
```

```{r}

par(mfrow=c(2,2),mar=c(4,4,1,1))
acf(mh$beta0[-burn.in],xlab=expression(beta0))
acf(mh$beta1[-burn.in],xlab=expression(beta1))
acf(mh$log_sigma[-burn.in],xlab=expression(log(sigma)))
acf(mh$log_sigma_b[-burn.in],xlab=expression(log(sigma_b)))
```

calculate the effective sample size for each paramater and see that the effective smaple size for $\log(\sigma)$ are about half the size than those for $\beta_0$, $\beta_1$ and $\sigma_b$.  
```{r}
# note we do discard burn-in iterations
n.eff <- c(0,0,0,0)
###
autocor <- acf(mh$beta0[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[1] <- nsteps/t.eff
###
autocor <- acf(mh$beta1[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[2] <- nsteps/t.eff
###
autocor <- acf(mh$log_sigma[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[3] <- nsteps/t.eff
###
autocor <- acf(mh$log_sigma_b[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[4] <- nsteps/t.eff

# t.eff is the integrated autocrrelation length
n.eff
```


We check for correlation between the parameters by plotting graphs... 

```{r}
par(mfrow=c(3,2),mar=c(4,4,1,1))
# For visualization purposes we take a random sample
# of the iterations retained (after discarding burn-in)
samp<-sample((1:nsteps)[-(burn.in)],nsteps/2)

plot(mh$beta0[samp],mh$beta1[samp],xlab=expression(beta_0),ylab=expression(beta_1),pch=".",cex=0.1)

plot(mh$beta0[samp],mh$log_sigma[samp],xlab=expression(beta_0),ylab=expression(log(sigma)),pch=".",cex=0.1)

plot(mh$beta0[samp],mh$log_sigma_b[samp],xlab=expression(beta_0),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
plot(mh$beta1[samp],mh$log_sigma[samp],xlab=expression(beta_1),ylab=expression(log(sigma)),pch=".",cex=0.1)
plot(mh$beta1[samp],mh$log_sigma_b[samp],xlab=expression(beta_1),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
plot(mh$log_sigma[samp],mh$log_sigma_b[samp],xlab=expression(log(sigma)),ylab=expression(log(sigma_b)),pch=".",cex=0.1)
```


* Tune your new Metropolis-Hastings sampler by tuning the appropriate parameter in the proposal distribution.

* In general, you should give substantial empirical evidence that the Markov chain constructed has converged to its stationary limiting distribution.

* Produce plots to learn about the shape of the marginal posterior densities of the parameters. You should also investigate the correlation between parameters.

* Compute a 95% posterior probability interval for the intervention effect $\beta_1$. What conclusions can you draw?



\newpage

# Fatigue of materials

Materials which are subject to cyclic loading are susceptible to cumulative damage and eventual failure through an irreversible process called \textit{fatigue.} Prediction of such fatigue  is important in mechanical and structural Engineering practice.

The fatigue characteristics of materials are established through fatigue tests which are usually performed on small flat plates called coupons. The levels of stress (force per unit area) as well as the number of cycles to failure are recorded for each test. While the stress levels $S_i$ are controlled during the test,  the number of cycles to failure $N_i$ exhibits a random behaviour due to inherent microstructural inhomogeneity in the material properties and also due to uncontrolled differences  in test conditions. 

When a test is stopped before the coupon fails,  then the coupon is marked as a  \emph{runout}. Runouts can be treated in terms of the likelihood exactly in the same way as censored observations.

Let   $N_i$ denote the number cycles until failure in coupon $i$ and $s_i$ be the corresponding stress level (in Mega Pascals) applied.  We model $N_i$ as continuous using the following probability model:

$$N_i=\alpha\,(s_i-\gamma)^\delta\,\epsilon_i\,,\qquad \mbox{where}\quad s_i>\gamma$$
and  $\epsilon_i$ is a random error such that

$$\epsilon_i\sim \mbox{Weibull}(\mbox{shape}=1/\sigma,\mbox{scale}=1)$$ 
The constants $\alpha>0$, $\delta\in R$, $\gamma>0$  and $\sigma>0$ are unknown parameters. Empirical results suggest that coupons tested below the stress level $\gamma$ will never fail. The unknown parameter $\gamma$ is therefore called the \textbf{fatigue limit.} Consider the following data obtained in a series of 26 tests to study the fatigue of a nickel base supperalloy
 
```{r}
fatigue<- read.table("http://people.bath.ac.uk/kai21/ASI/fatigue.txt")
head(fatigue)
```


```{r,echo=FALSE,message=FALSE,fig.width=7,fig.height=4}
library(ggplot2)
library(dplyr)
library(forcats)
fatigue<-fatigue %>% mutate(ro=factor(ro)) %>% mutate(ro=fct_recode(ro,"Failure"="0","Runout"="1"))
p1 <- ggplot(fatigue, aes(N, s,colour=ro)) + geom_point() +  labs(x = "Number of Cycles (Log scale)", y = "Stress (MPa)") + scale_y_continuous(breaks = c(80, 100, 120,140))
p2<-p1 + scale_x_log10(breaks = scales::trans_breaks("log10", function(x) 10^x),labels = scales::trans_format("log10", scales::math_format(10^.x)))
p2+scale_color_manual(values=c(Failure="black",Runout="grey"))
```

The dataset only contains 3 variables: the number of cycles is in \texttt{N}, the stress levels is in \texttt{s} and the runout indicator is in \texttt{ro}. 

\newpage

 1. Use \texttt{optim} to find the maximum likelihood estimate of $\bm{\theta}=(\log(\alpha),\delta,\log(\sigma))^\top$ for an arbitary value of $\gamma$ that you can choose. The chosen value of $\gamma>0$ should be below the minimum of all stress values that gave rise to a failure, that is, ignoring runouts. Compute  asymptotic 95\% confidence intervals for each of the unknown parameters in $\bm{\theta}$. Now try with different values of $\gamma$ and comment on the sensitivity of the results to the chosen value of $\gamma$. 
 
 2. How would you estimate the vector of unknown parameters $\bm{\theta_*}=(\log(\alpha),\delta,\log(\sigma),\gamma)^\top$?
 
3. Of great engineering interest are the lower quantiles of the fatigue as a function of stress. We will focus on the lower 10\%  quantile of $N$ which in this case is given by 
$$N_{0.1}=\alpha\,(s_i-\gamma)^\delta\,z^\sigma_{0.1}$$
where $z_{0.1}$ is the lower 10\%  quantile of a  Weibull with unit shape and unit scale, that is, an exponential distribution with unit rate. By plugging in estimated values of the unknown parameters, estimate the lower 10\%  quantile curve of $N$ (curve as a function of the stress levels $s$) and plot it together with the data. In the same plot, also add the corresponding median curve which is obtained by using $z_{0.5}$ above, which is the median of an exponential distribution with unit rate.

 
 4. The above model assumes that all coupons have the same unknown  fatigue limit $\gamma$. Consider now the following random effects model that allows the fatigue limit to be different for each coupon. This is achived by modelling the fatigue limit as an unobserved random variable $\Gamma$. For coupon $i$,  the conditional distribution of the number of cycles to failure $N_i$ given that  $\Gamma_i=\gamma_i<s_i$ that is, given that the realisation of the fatigue limit for that coupon is below the applied stress level is given by
 \begin{equation}
 N_i|\Gamma_i=\gamma_i<s_i\sim \mbox{Weibull}(\mbox{shape}=1/\sigma,\mbox{scale}=\alpha\,(s_i-\gamma_i)^\delta)
 \end{equation}
We will assume that $\Gamma_1,\ldots,\Gamma_{26}$ are iid 
 \begin{equation}
\mbox{Weibull}(\mbox{shape}=1/\sigma_{\gamma},\mbox{scale}=\exp(\mu_\gamma))
\end{equation}
where $\mu_{\gamma}\in R,\sigma_{\gamma}>0$ are unknown parameters.
Let $\bm{b}=(\gamma_1,\ldots,\gamma_{26})^\top$ and the vector of unknown paramaters is now given by $\bm{\theta}^\top=(\log(\alpha),\delta,\log(\sigma),\mu_\gamma,\log(\sigma_{\gamma}))$ We will assume the following priors: $\log(\alpha),\delta,\log(\sigma),\mu_{\gamma}$ are indpendent and with improper uniform priors while $\sigma_{\gamma}$ is exponential with rate 5 and independent of $\log(\alpha),\delta,\log(\sigma)$ and $\mu_{\gamma}$. Use a  Metropolis-Hastings algorithm to sample from the posterior distribution of $\bm{\theta}$ and the random effects $\bm{b}$. Apart from the guideline points in the previous section, you should consider using uniform (over finite intervals) proposal distributions for moving around the allowed space of the random effects. 
```{r}
split_fatigue <- split(fatigue, fatigue$ro)
broke=as.data.frame(split_fatigue[[1]])
runoff=as.data.frame(split_fatigue[[2]])

head(broke)
head(runoff)

```

```{r}

likelihood_broke=function(log_alpha, delta, log_sigma,mu_gamma,log_sigma_gamma,  N,s){
  likelihood=0
  gamma=0
  while(gamma<s){ 
  likelihood=likelihood+ dweibull(N, 1/exp(log_sigma), exp(log_alpha)*(s-gamma)^delta)*pweibull(gamma, 1/exp(log_sigma_gamma), exp(mu_gamma))
    gamma=gamma+1
  }
  return(likelihood)
}

likelihood_runoff=function(log_alpha, delta, log_sigma,mu_gamma,log_sigma_gamma,  N,s){
  likelihood=0
  gamma=0
  while(gamma<s){
    likelihood=likelihood+ pweibull(N, 1/exp(log_sigma), exp(log_alpha)*(s-gamma)^delta)*pweibull(gamma, 1/exp(log_sigma_gamma), exp(mu_gamma))
    gamma=gamma+1
  }
  return(likelihood)
}
# function to compute log posterior 
log.post <- function(log_alpha, delta, log_sigma,mu_gamma,log_sigma_gamma,  broke, runoff) { 
  N_broke=broke$N
  s_broke=broke$s
 
  N_runoff=runoff$N
  s_runoff=runoff$s
  
  
  log_pi0_log_alpha=log(1)
  log_pi0_delta=  log(1)
  log_pi0_log_sigma=log(1)
  log_pi0_mu_gamma= log(1)
  log_pi0_log_sigma_gamma=log(5*exp(-5*exp(log_sigma_gamma)))
  
   # we add the priors since we assume they are independent 
  log_pi0<- log_pi0_log_alpha+log_pi0_delta+log_pi0_log_sigma+log_pi0_mu_gamma+log_pi0_log_sigma_gamma
  # Now the log likelihood
  log.lik=0
  for(i in 1:length(broke$s)){
    log.lik=log.lik+log(likelihood_broke(log_alpha, delta, log_sigma,mu_gamma,log_sigma_gamma,  broke$N[i],broke$s[i]))
  }
  for(i in 1:length(runoff$s)){
    log.lik=log.lik+log(likelihood_runoff(log_alpha, delta, log_sigma,mu_gamma,log_sigma_gamma,  runoff$N[i],runoff$s[i]))
  }
  # now the log posterior = log likelihood +log prior 
  return(log.lik+log_pi0)
}
log.post(3,3,3,3,3,broke, runoff)


```

```{r}

# function to compute log posterior 
log.post_new <- function(log_alpha, delta, log_sigma,mu_gamma,log_sigma_gamma,  broke, runoff) { 
  N_broke=broke$N
  s_broke=broke$s
 
  N_runoff=runoff$N
  s_runoff=runoff$s
  
  
  log_pi0_log_alpha=log(1)
  log_pi0_delta=  log(1)
  log_pi0_log_sigma=log(1)
  log_pi0_mu_gamma= log(1)
  log_pi0_log_sigma_gamma=log(5*exp(-5*exp(log_sigma_gamma)))
  
   # we add the priors since we assume they are independent 
  log_pi0<- log_pi0_log_alpha+log_pi0_delta+log_pi0_log_sigma+log_pi0_mu_gamma+log_pi0_log_sigma_gamma
  # Now the log likelihood
  gamma_broke=rweibull(length(broke$s), 1/exp(log_sigma_gamma), exp(mu_gamma))
    
  gamma_runoff=rweibull(length(runoff$s),1/exp(log_sigma_gamma), exp(mu_gamma))
  check=1
  log.lik=0
  for(i in 1:length(broke$s)){
    if(broke$s[i]- gamma_broke[i]>0){
      log.lik=log.lik+log(dweibull(broke$N[i],1/exp(log_sigma), exp(log_alpha)*(broke$s[i]-gamma_broke[i])**delta))
      
        
    }else{
          check=check*0
        }
    
  }
   for(i in 1:length(runoff$s)){
    if(broke$s[i]- gamma_runoff[i]>0){
      log.lik=log.lik+log(pweibull(runoff$N[i],1/exp(log_sigma), exp(log_alpha)*(runoff$s[i]-gamma_runoff[i])**delta))
      
        
    }else{
          check=check*0
        }
    
  }
  

  # now the log posterior = log likelihood +log prior 
  return(log.lik+log_pi0)
}
log.post_new(-1,2,1,1,1,broke, runoff=runoff)


```


```{r}
nsteps=10000





burn.in=1000

MH<-function(log_alpha0, delta0, log_sigma0,mu_gamma0,log_sigma_gamma0, range_log_alpha, range_delta, range_log_sigma,range_mu_gamma,range_log_sigma_gamma, nsteps=nsteps){ 
  

  
  accept <- rep(0,nsteps)
  log_alpha <- rep(0,nsteps)
  delta=rep(0,nsteps)
  log_sigma=rep(0,nsteps)
  mu_gamma=rep(0,nsteps)
  log_sigma_gamma=rep(0,nsteps)
  
  

  log_alpha[1] <- log_alpha0
  delta[1]=delta0
  log_sigma[1]=log_sigma0
  mu_gamma[1]=mu_gamma0
  log_sigma_gamma[1]=log_sigma_gamma0
  
  lp0 <- log.post(log_alpha0, delta0, log_sigma0,mu_gamma0,log_sigma_gamma0,  broke, runoff)
  print(lp0)
  for (i in 2:nsteps){
    current_log_alpha=log_alpha[i-1]
    current_delta=delta[i-1]
    current_log_sigma=log_sigma[i-1]
    current_mu_gamma=mu_gamma[i-1]
    current_log_sigma_gamma=log_sigma_gamma[i-1]
    
    proposed_log_alpha=current_log_alpha+sample(c(-range_log_alpha:range_log_alpha),1) 
    proposed_delta=current_delta+sample(c(- range_delta :range_delta),1) 
    proposed_log_sigma=current_log_sigma+sample(c(- range_log_sigma:range_log_sigma),1) 
    proposed_mu_gamma=current_mu_gamma+sample(c(-range_mu_gamma:range_mu_gamma),1) 
    proposed_log_sigma_gamma=current_log_sigma_gamma+sample(c(-range_log_sigma_gamma:range_log_sigma_gamma),1) 
    
     if (pweibull(80.3,1/exp(proposed_log_sigma_gamma), exp(proposed_mu_gamma) ) <=0.05 ) {
      # definitely reject the whole vector if it is not possible for gamma<s
      log_alpha[i] <- current_log_alpha
      delta[i]=current_delta
      log_sigma[i]=current_log_sigma
      mu_gamma[i]=current_mu_gamma
      log_sigma_gamma[i]=current_log_sigma_gamma
      
    } else {
    
   #  proceed with Metropolis-Hastings step
        lp1 <- log.post(proposed_log_alpha,proposed_delta, proposed_log_sigma, proposed_mu_gamma, proposed_log_sigma_gamma, broke, runoff)
        acc <- exp(min(0,lp1-lp0))
        #print(acc)
        if(is.finite(acc)){
        if (runif(1) <= acc) { # accept
          
          log_alpha[i] <- proposed_log_alpha
          delta[i]=proposed_delta
          log_sigma[i]=proposed_log_sigma
          mu_gamma[i]=proposed_mu_gamma
          log_sigma_gamma[i]=proposed_log_sigma_gamma
  

          accept[i] <- 1
          #print('yes')
          lp0 <- lp1 ## Keep ll0 in sync with th
        } else { ## reject
          log_alpha[i] <- current_log_alpha
          delta[i]=current_delta
          log_sigma[i]=current_log_sigma
          mu_gamma[i]=current_mu_gamma
          log_sigma_gamma[i]=current_log_sigma_gamma
            
            
            lp1 <- lp0 ## Keep ll1 in sync with th 
        }
        } else{## reject
          log_alpha[i] <- current_log_alpha
          delta[i]=current_delta
          log_sigma[i]=current_log_sigma
          mu_gamma[i]=current_mu_gamma
          log_sigma_gamma[i]=current_log_sigma_gamma
            
            
            lp1 <- lp0 ## Keep ll1 in sync with th 
      
      
    }
  } 

  } 
   list(log_alpha=log_alpha, delta=delta, log_sigma=log_sigma,mu_gamma=mu_gamma,log_sigma_gamma=log_sigma_gamma, ar=mean(accept))
}


mh=MH(-1,2,1,1,1,1,1,1,1,1,nsteps = nsteps)
mh$ar
```

```{r}
show.plot<- (nsteps-1000):nsteps
par(mfrow=c(5,1), mar=c(2,2,1,1))
plot(mh$log_alpha[show.plot],type="l",ylab=expression(log_alpha))
plot(mh$delta[show.plot],type="l",ylab=expression(delta))
plot(mh$log_sigma[show.plot],type="l",ylab=expression(log(sigma)))
plot(mh$mu_gamma[show.plot],type="l",ylab=expression(mu_gamma))
plot(mh$log_sigma_gamma[show.plot],type="l",ylab=expression(log(sigma_gamma)))
```

```{r}

par(mfrow=c(3,2),mar=c(4,4,1,1))
acf(mh$log_alpha[-burn.in],xlab=expression(log(alpha)))
acf(mh$delta[-burn.in],xlab=expression(delta))
acf(mh$log_sigma[-burn.in],xlab=expression(log(sigma)))
acf(mh$mu_gamma[-burn.in],xlab=expression(mu_gamma))
acf(mh$log_sigma_gamma[-burn.in],xlab=expression(log(sigma_gamma)))
```

calculate the effective sample size for each paramater and see that the effective smaple size for $\log(\sigma)$ are about half the size than those for $\beta_0$, $\beta_1$ and $\sigma_b$.  
```{r}
# note we do discard burn-in iterations
n.eff <- c(0,0,0,0,0)
###
autocor <- acf(mh$log_alpha[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[1] <- nsteps/t.eff
###
autocor <- acf(mh$delta[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[2] <- nsteps/t.eff
###
autocor <- acf(mh$log_sigma[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[3] <- nsteps/t.eff
###
autocor <- acf(mh$mu_gamma[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[4] <- nsteps/t.eff

autocor <- acf(mh$log_sigma_gamma[-(burn.in)],plot=FALSE)
t.eff <- 2*sum(autocor[[1]]) - 1
n.eff[4] <- nsteps/t.eff

# t.eff is the integrated autocrrelation length
n.eff
```


We check for correlation between the parameters by plotting graphs... 

```{r}
par(mfrow=c(5,2),mar=c(2,2,1,1))
# For visualization purposes we take a random sample
# of the iterations retained (after discarding burn-in)
samp<-sample((1:nsteps)[-(burn.in)],nsteps/2)

plot(mh$log_alpha[samp],mh$delta[samp],xlab=expression(log(alpha)),ylab=expression(delta),pch=".",cex=0.1)

plot(mh$log_alpha[samp],mh$log_sigma[samp],xlab=expression(log(alpha)),ylab=expression(log(sigma)),pch=".",cex=0.1)

plot(mh$log_alpha[samp],mh$log_sigma_gamma[samp],xlab=expression(log(alpha)),ylab=expression(log(sigma_gamma)),pch=".",cex=0.1)
plot(mh$delta[samp],mh$log_sigma[samp],xlab=expression(delta),ylab=expression(log(sigma)),pch=".",cex=0.1)
plot(mh$delta[samp],mh$log_sigma_gamma[samp],xlab=expression(delta),ylab=expression(log(sigma_gamma)),pch=".",cex=0.1)
plot(mh$log_sigma[samp],mh$log_sigma_gamma[samp],xlab=expression(log(sigma)),ylab=expression(log(sigma_gamma)),pch=".",cex=0.1)

plot(mh$mu_gamma[samp],mh$log_alpha[samp],xlab=expression(mu_gamma),ylab=expression(log(alpha)),pch=".",cex=0.1)
plot(mh$mu_gamma[samp],mh$delta[samp],xlab=expression(mu_gamma),ylab=expression(delta),pch=".",cex=0.1)
plot(mh$mu_gamma[samp],mh$log_sigma[samp],xlab=expression(mu_gamma),ylab=expression(log(sigma)),pch=".",cex=0.1)
plot(mh$mu_gamma[samp],mh$log_sigma_gamma[samp],xlab=expression(mu_gamma),ylab=expression(log(sigma_gamma)),pch=".",cex=0.1)

```
